# -*- coding: utf-8 -*-
"""Protein.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jwHJxs2izgl-y2AMp4csk9sqUI9C0pbQ
"""

# ===============================
# ðŸ§¬ ProtGPT2 + LoRA Fine-Tuning + Evaluation + Loss + Plots (Fixed Labels)
# ===============================

!pip install -q transformers peft accelerate datasets gradio biopython matplotlib

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from datasets import Dataset
from peft import LoraConfig, get_peft_model
import gradio as gr
from Bio.SeqUtils.ProtParam import ProteinAnalysis
import matplotlib.pyplot as plt

# -------------------------------
# 1) Load ProtGPT2 + Tokenizer
# -------------------------------
MODEL_NAME = "nferruz/ProtGPT2"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token  # fix padding

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto"
)

# -------------------------------
# 2) Apply LoRA adapters
# -------------------------------
config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["c_proj","c_attn","q_attn"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, config)
print("Trainable parameters:", sum(p.numel() for p in model.parameters() if p.requires_grad))

# -------------------------------
# 3) Build Local Protein Dataset
# -------------------------------
sequences = [
    "MKTFFVVALVVLATGVHSAG",
    "MKADKSELVQKAKLAEQAERYDDMAAAMKAVTEQGHELS",
    "MSEQNNTEMTFQIQRIYTKDISFEAPNAPHVFQKDW",
    "MADEEKLPPGWEKRMSRSSGRVYYFNHITNASQWERPSGN",
    "MGDVEKGKKIFIMKCSQCHTVEKGGKHKTGPNEKG"
]
dataset = Dataset.from_dict({"text": sequences})

def tokenize_function(examples):
    # Encode input_ids
    enc = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512
    )
    # For causal LM, labels = input_ids
    enc["labels"] = enc["input_ids"].copy()
    return enc

tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# -------------------------------
# 4) Training setup (No wandb)
# -------------------------------
training_args = TrainingArguments(
    output_dir="./protgpt2-lora",
    evaluation_strategy="no",
    save_strategy="epoch",
    logging_dir=None,       # disable wandb
    report_to=[],           # disable all trackers
    logging_steps=1,
    learning_rate=2e-4,
    per_device_train_batch_size=2,
    num_train_epochs=1,
    fp16=True,
    save_total_limit=2,
    push_to_hub=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer
)

# -------------------------------
# 5) Train and track loss
# -------------------------------
trainer.train()
loss_history = [x["loss"] for x in trainer.state.log_history if "loss" in x]

# -------------------------------
# 6) Protein Evaluation Functions
# -------------------------------
def evaluate_protein(seq):
    try:
        analysed_seq = ProteinAnalysis(seq)
        mw = analysed_seq.molecular_weight()
        instability = analysed_seq.instability_index()
        gravy = analysed_seq.gravy()
        aromaticity = analysed_seq.aromaticity()
        return {
            "length": len(seq),
            "molecular_weight": mw,
            "instability_index": instability,
            "gravy": gravy,
            "aromaticity": aromaticity
        }
    except Exception as e:
        return {"error": str(e)}

def generate_and_evaluate(prompt, max_length, temperature, top_p, n_samples=5):
    results = []
    for _ in range(n_samples):
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            eos_token_id=tokenizer.eos_token_id
        )
        seq = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(" ", "")
        evals = evaluate_protein(seq)
        results.append({"sequence": seq, **evals})
    return results

# -------------------------------
# 7) Multi-Property + Training Loss Plot
# -------------------------------
def plot_all_properties_and_loss(results, loss_history):
    props = ["molecular_weight", "instability_index", "gravy", "aromaticity"]
    titles = ["Molecular Weight", "Instability Index", "GRAVY", "Aromaticity"]

    plt.figure(figsize=(16, 12))

    # Protein properties
    for i, prop in enumerate(props, 1):
        values = [r[prop] for r in results if prop in r]
        plt.subplot(3, 2, i)
        plt.hist(values, bins=10, edgecolor="black")
        plt.title(titles[i-1])
        plt.xlabel(prop)
        plt.ylabel("Count")
        plt.grid(True)

    # Training loss
    plt.subplot(3, 1, 3)
    plt.plot(loss_history, marker='o')
    plt.title("Training Loss")
    plt.xlabel("Step")
    plt.ylabel("Loss")
    plt.grid(True)

    plt.tight_layout()
    plt.savefig("properties_and_loss.png")
    return "properties_and_loss.png"

# -------------------------------
# 8) Gradio UI
# -------------------------------
def gradio_interface(prompt, max_length, temperature, top_p):
    results = generate_and_evaluate(prompt, max_length, temperature, top_p, n_samples=5)
    plot_path = plot_all_properties_and_loss(results, loss_history)

    table = "\n\n".join([f"Seq: {r['sequence'][:50]}...\n"
                         f"Length: {r['length']}, MW: {r['molecular_weight']:.2f}, "
                         f"Instability: {r['instability_index']:.2f}, "
                         f"GRAVY: {r['gravy']:.2f}, "
                         f"Aromaticity: {r['aromaticity']:.2f}"
                         for r in results])
    return table, plot_path

with gr.Blocks() as demo:
    gr.Markdown("## ðŸ§¬ ProtGPT2 + LoRA Protein Generator + Evaluator + Training Loss")
    with gr.Row():
        with gr.Column():
            prompt = gr.Textbox(label="Prompt", value="MKT...")
            max_length = gr.Slider(50, 1024, 256, step=10, label="Max Length")
            temperature = gr.Slider(0.1, 2.0, 1.0, step=0.1, label="Temperature")
            top_p = gr.Slider(0.1, 1.0, 0.9, step=0.05, label="Top-p")
            generate_btn = gr.Button("Generate & Evaluate")
        with gr.Column():
            output_text = gr.Textbox(label="Generated Proteins + Properties", lines=12)
            output_plot = gr.Image(label="Properties + Training Loss")

    generate_btn.click(gradio_interface, [prompt, max_length, temperature, top_p], [output_text, output_plot])

demo.launch(share=True)

def plot_all_protein_properties_extended(results, loss_history):
    import pandas as pd
    import seaborn as sns
    import matplotlib.pyplot as plt
    import numpy as np

    # Convert results to DataFrame
    df = pd.DataFrame(results)
    numeric_props = ["length", "molecular_weight", "instability_index", "gravy", "aromaticity"]

    # -----------------------
    # 1) Histograms + KDE
    # -----------------------
    plt.figure(figsize=(20, 15))
    for i, prop in enumerate(numeric_props, 1):
        plt.subplot(3, 2, i)
        sns.histplot(df[prop], bins=10, kde=True, color='skyblue')
        plt.title(f"Histogram & KDE: {prop}")
        plt.xlabel(prop)
        plt.ylabel("Count")
        plt.grid(True)

    # -----------------------
    # 2) Scatter plots
    # -----------------------
    plt.subplot(3, 2, 6)
    plt.scatter(df["gravy"], df["instability_index"], s=60, c='red', label="GRAVY vs Instability")
    plt.scatter(df["molecular_weight"], df["aromaticity"], s=60, c='green', label="MW vs Aromaticity")
    plt.xlabel("X-axis properties")
    plt.ylabel("Y-axis properties")
    plt.title("Scatter: GRAVY vs Instability & MW vs Aromaticity")
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig("protein_extended_plots.png")

    # -----------------------
    # 3) Violin plots
    # -----------------------
    plt.figure(figsize=(12,6))
    sns.violinplot(data=df[numeric_props], palette="Pastel1")
    plt.title("Violin Plots of Protein Properties")
    plt.grid(True)
    plt.savefig("protein_violin.png")

    # -----------------------
    # 4) Pairwise scatter matrix
    # -----------------------
    sns.pairplot(df[numeric_props], kind='scatter', diag_kind='kde', plot_kws={'s':50, 'alpha':0.6})
    plt.savefig("protein_pairplot.png")

    # -----------------------
    # 5) Correlation heatmap
    # -----------------------
    plt.figure(figsize=(8,6))
    corr = df[numeric_props].corr()
    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title("Correlation Heatmap")
    plt.savefig("protein_correlation.png")

    # -----------------------
    # 6) CDF plots
    # -----------------------
    plt.figure(figsize=(12,6))
    for prop in numeric_props:
        sorted_vals = np.sort(df[prop])
        cdf = np.arange(len(sorted_vals)) / float(len(sorted_vals))
        plt.plot(sorted_vals, cdf, marker='o', label=prop)
    plt.title("Cumulative Distribution Functions (CDF) of Properties")
    plt.xlabel("Property Value")
    plt.ylabel("CDF")
    plt.legend()
    plt.grid(True)
    plt.savefig("protein_cdf.png")

    # -----------------------
    # 7) Training Loss Plot
    # -----------------------
    plt.figure(figsize=(8,4))
    plt.plot(loss_history, marker='o', color='purple')
    plt.title("Training Loss Over Steps")
    plt.xlabel("Step")
    plt.ylabel("Loss")
    plt.grid(True)
    plt.savefig("training_loss_extended.png")